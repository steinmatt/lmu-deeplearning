{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC 598 : Deep Learning Applications \n",
    "# Project 1 - Predicting Housing Prices \n",
    "### By: Matthew Stein and Ethan Tsao \n",
    "### February 10, 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "Data is imported via pandas, and processed via numpy. Working with Keras, all data is processed as arrays in numpy. To simplify model learning, feature-wise normalization is performed. Note that feature-wise normalization is calc'ed using the mean and std of the training set. \n",
    "\n",
    "The test set was chosen to be 15% of the overall data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SquareFootageStructure  LotSquareFootage  YearBuilt  Bedrooms  \\\n",
      "0                     1549.0            5825.0     1974.0       3.0   \n",
      "1                     1196.0            7900.0     1981.0       3.0   \n",
      "3                     3884.0           16013.0     1978.0       3.0   \n",
      "6                     1164.0            6611.0     1961.0       3.0   \n",
      "12                    2766.0            7331.0     1986.0       4.0   \n",
      "...                      ...               ...        ...       ...   \n",
      "9989                  2300.0            6547.0     1924.0       4.0   \n",
      "9990                  1545.0            2178.0     1967.0       2.0   \n",
      "9991                  1404.0            7841.0     1964.0       2.0   \n",
      "9993                   900.0            6000.0     1924.0       2.0   \n",
      "9999                  2865.0            7841.0     2005.0       4.0   \n",
      "\n",
      "      BathsTotal  field_StoriesTotal  field_PostalCode  ListPrice  \n",
      "0            2.0                 1.0           92624.0     3300.0  \n",
      "1            2.0                 1.0           92316.0     1600.0  \n",
      "3            3.0                 1.0           90274.0  2599000.0  \n",
      "6            2.0                 1.0           91732.0   499999.0  \n",
      "12           3.0                 2.0           91750.0   929800.0  \n",
      "...          ...                 ...               ...        ...  \n",
      "9989         2.0                 1.0           90046.0  2445000.0  \n",
      "9990         2.0                 1.0           92264.0   320000.0  \n",
      "9991         2.0                 1.0           92586.0   228500.0  \n",
      "9993         1.0                 1.0           90019.0     2700.0  \n",
      "9999         3.0                 2.0           92585.0   399990.0  \n",
      "\n",
      "[6370 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('data/resd_data.xlsx')\n",
    "length = data.shape[0]\n",
    "\n",
    "# Remove all rows with null points in dataset \n",
    "updated_data = updated_data[np.isfinite(updated_data)]\n",
    "\n",
    "# Remove all rows with \"0\" for the Postal Code, \"0\" for Year Built, \"0\" for Lot Square Footage, and \"0\" for List Price\n",
    "updated_data = updated_data[updated_data.field_PostalCode != 0 ]\n",
    "updated_data = updated_data[updated_data.YearBuilt != 0]\n",
    "updated_data = updated_data[updated_data.LotSquareFootage != 0]\n",
    "updated_data = updated_data[updated_data.ListPrice != 0]\n",
    "\n",
    "# Update Index column \n",
    "\n",
    "print(updated_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = updated_data.loc[:5000,:'field_PostalCode'].values.tolist()\n",
    "train_data = np.asarray(train_data) \n",
    "train_targets = updated_data.loc[:5000,'ListPrice'].values.tolist()\n",
    "train_targets = np.asarray(train_targets) \n",
    "\n",
    "test_data = updated_data.loc[5001:,:'field_PostalCode'].values.tolist()\n",
    "test_data = np.asarray(test_data) \n",
    "test_targets = updated_data.loc[5001:,'ListPrice'].values.tolist()\n",
    "test_targets = np.asarray(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection \n",
    "\n",
    "As is standard for neural networks - and regression models, in particular - a Sequential model was chosen. The model has 3 hidden layers, with 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "(0, 7)\n",
      "Train on 2384 samples, validate on 794 samples\n",
      "Epoch 1/100\n",
      "2384/2384 [==============================] - 1s 383us/step - loss: 3396892369060.9395 - mae: 881247.1250 - val_loss: 2162124797482.5591 - val_mae: 851137.5625\n",
      "Epoch 2/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 3396345325554.2549 - mae: 881060.0000 - val_loss: 2161294233342.0654 - val_mae: 850781.3125\n",
      "Epoch 3/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 3394859532947.7583 - mae: 880509.6875 - val_loss: 2159332167876.0303 - val_mae: 849964.5000\n",
      "Epoch 4/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 3391122821972.1880 - mae: 879266.9375 - val_loss: 2155169980312.8262 - val_mae: 848365.5625\n",
      "Epoch 5/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 3384597323281.1812 - mae: 877301.8750 - val_loss: 2148127463264.0806 - val_mae: 846160.8750\n",
      "Epoch 6/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 3374307472157.2080 - mae: 874545.8125 - val_loss: 2136422296207.1536 - val_mae: 842956.6250\n",
      "Epoch 7/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 3357123786710.7651 - mae: 870319.9375 - val_loss: 2118685562843.8892 - val_mae: 838305.6875\n",
      "Epoch 8/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 3329132315173.7988 - mae: 864271.8750 - val_loss: 2092972421460.4736 - val_mae: 831587.1250\n",
      "Epoch 9/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 3295774369338.4160 - mae: 856292.5625 - val_loss: 2058535616718.3477 - val_mae: 822591.8125\n",
      "Epoch 10/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 3251232887636.1880 - mae: 845405.7500 - val_loss: 2017436877006.3477 - val_mae: 811726.0000\n",
      "Epoch 11/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 3190663642916.0806 - mae: 831226.6875 - val_loss: 1960526068150.4888 - val_mae: 796538.3750\n",
      "Epoch 12/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 3114607852578.3623 - mae: 813006.0000 - val_loss: 1891870126007.7783 - val_mae: 777635.0625\n",
      "Epoch 13/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 3024722261424.9663 - mae: 790284.8125 - val_loss: 1813593736942.5894 - val_mae: 754978.1250\n",
      "Epoch 14/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 2926360953711.6777 - mae: 763620.8750 - val_loss: 1728902760298.3979 - val_mae: 729172.3750\n",
      "Epoch 15/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 2821051059206.8726 - mae: 733350.3125 - val_loss: 1641244728497.9749 - val_mae: 700415.2500\n",
      "Epoch 16/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 2707893792795.4902 - mae: 700388.5625 - val_loss: 1546417785714.1360 - val_mae: 668467.9375\n",
      "Epoch 17/100\n",
      "2384/2384 [==============================] - 0s 105us/step - loss: 2576004167638.7651 - mae: 663697.1875 - val_loss: 1447172994973.9849 - val_mae: 634843.1875\n",
      "Epoch 18/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 2445619866685.8525 - mae: 630928.8750 - val_loss: 1349908371584.9673 - val_mae: 602855.6875\n",
      "Epoch 19/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 2336957337572.5098 - mae: 603284.1875 - val_loss: 1266116769363.8286 - val_mae: 576709.8750\n",
      "Epoch 20/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 2219392427248.5371 - mae: 581623.0625 - val_loss: 1196397817618.7002 - val_mae: 559467.5625\n",
      "Epoch 21/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 2133330051181.9595 - mae: 571299.0625 - val_loss: 1146345427222.5693 - val_mae: 551856.0625\n",
      "Epoch 22/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 2066091575007.3557 - mae: 566945.8750 - val_loss: 1107180692343.2947 - val_mae: 550108.0000\n",
      "Epoch 23/100\n",
      "2384/2384 [==============================] - 0s 102us/step - loss: 2013314695497.8792 - mae: 568482.9375 - val_loss: 1084763863973.7229 - val_mae: 552436.5000\n",
      "Epoch 24/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 1973184873657.5571 - mae: 573648.6875 - val_loss: 1069721690751.6776 - val_mae: 555927.6250\n",
      "Epoch 25/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1947296430808.4834 - mae: 578344.4375 - val_loss: 1060551166669.0580 - val_mae: 559382.3750\n",
      "Epoch 26/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1929636689219.0066 - mae: 583586.5000 - val_loss: 1055446700929.6121 - val_mae: 562035.2500\n",
      "Epoch 27/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1912363106194.0405 - mae: 586482.9375 - val_loss: 1051141302803.3451 - val_mae: 564605.0000\n",
      "Epoch 28/100\n",
      "2384/2384 [==============================] - 0s 102us/step - loss: 1897131802184.1611 - mae: 586204.3125 - val_loss: 1049472047728.2015 - val_mae: 567470.0000\n",
      "Epoch 29/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1889217747617.5034 - mae: 590617.9375 - val_loss: 1046499876306.8615 - val_mae: 566940.7500\n",
      "Epoch 30/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1880936409074.2549 - mae: 591208.0000 - val_loss: 1045149586677.0378 - val_mae: 567283.6875\n",
      "Epoch 31/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1874643616843.5974 - mae: 589606.8125 - val_loss: 1042148491470.3477 - val_mae: 565952.3750\n",
      "Epoch 32/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1869023890988.6711 - mae: 590103.9375 - val_loss: 1041326026019.4659 - val_mae: 566777.8750\n",
      "Epoch 33/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1862434854046.0671 - mae: 589859.6250 - val_loss: 1040044715670.8917 - val_mae: 566457.5000\n",
      "Epoch 34/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 1857835175578.6309 - mae: 591050.6875 - val_loss: 1037768064528.7657 - val_mae: 564585.0000\n",
      "Epoch 35/100\n",
      "2384/2384 [==============================] - 0s 114us/step - loss: 1852507433496.0537 - mae: 589417.3125 - val_loss: 1035496065864.8665 - val_mae: 563230.5625\n",
      "Epoch 36/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1849873351205.7988 - mae: 588348.4375 - val_loss: 1034309563038.6298 - val_mae: 562217.4375\n",
      "Epoch 37/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1845680508570.6309 - mae: 587446.6250 - val_loss: 1032405373330.3778 - val_mae: 560922.8750\n",
      "Epoch 38/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1841584937193.6643 - mae: 584688.1875 - val_loss: 1029712354935.9396 - val_mae: 559193.0000\n",
      "Epoch 39/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1838521326681.3423 - mae: 581730.5000 - val_loss: 1028592982062.4282 - val_mae: 558806.5625\n",
      "Epoch 40/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1834372988708.0806 - mae: 581378.5000 - val_loss: 1026923264892.4534 - val_mae: 558009.0000\n",
      "Epoch 41/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1831596101494.5505 - mae: 580945.3125 - val_loss: 1024805396706.9824 - val_mae: 556353.3125\n",
      "Epoch 42/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1829174113053.2080 - mae: 580039.8125 - val_loss: 1025142606004.5542 - val_mae: 556987.6250\n",
      "Epoch 43/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1825046427056.9666 - mae: 579594.7500 - val_loss: 1024594175210.7205 - val_mae: 556468.6250\n",
      "Epoch 44/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1821938696521.8792 - mae: 580632.7500 - val_loss: 1023255179356.8564 - val_mae: 555625.6875\n",
      "Epoch 45/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1818439871329.9329 - mae: 575799.1875 - val_loss: 1021133329322.8816 - val_mae: 554088.8750\n",
      "Epoch 46/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1814234179460.2952 - mae: 576864.1250 - val_loss: 1018422828787.7482 - val_mae: 551901.1250\n",
      "Epoch 47/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1813924992487.9463 - mae: 573872.5625 - val_loss: 1018889735315.0227 - val_mae: 552183.3125\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2384/2384 [==============================] - 0s 102us/step - loss: 1812819935451.9194 - mae: 573777.2500 - val_loss: 1020672807603.2645 - val_mae: 552889.5000\n",
      "Epoch 49/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1808994140819.7583 - mae: 573861.5625 - val_loss: 1019638528190.8716 - val_mae: 551696.5000\n",
      "Epoch 50/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 1806126272704.4297 - mae: 572643.4375 - val_loss: 1019085322967.3754 - val_mae: 551421.6875\n",
      "Epoch 51/100\n",
      "2384/2384 [==============================] - 0s 86us/step - loss: 1802640290919.0874 - mae: 572159.4375 - val_loss: 1018713349834.4786 - val_mae: 551055.5000\n",
      "Epoch 52/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 1801214756355.4363 - mae: 571685.6250 - val_loss: 1018217740478.8716 - val_mae: 550617.3125\n",
      "Epoch 53/100\n",
      "2384/2384 [==============================] - 0s 86us/step - loss: 1798168127144.3757 - mae: 570493.7500 - val_loss: 1017334055902.4685 - val_mae: 549539.8125\n",
      "Epoch 54/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1796946186322.4697 - mae: 569391.9375 - val_loss: 1017482623249.4105 - val_mae: 549037.8125\n",
      "Epoch 55/100\n",
      "2384/2384 [==============================] - 0s 87us/step - loss: 1795870522512.3220 - mae: 568790.6875 - val_loss: 1017582415180.7355 - val_mae: 548680.8125\n",
      "Epoch 56/100\n",
      "2384/2384 [==============================] - 0s 87us/step - loss: 1793940566483.3289 - mae: 568026.5000 - val_loss: 1017387700213.6826 - val_mae: 548297.6250\n",
      "Epoch 57/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1792124537532.9934 - mae: 568323.0625 - val_loss: 1018433172539.3250 - val_mae: 548475.5000\n",
      "Epoch 58/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1789425083522.5771 - mae: 567209.6875 - val_loss: 1018042913376.7255 - val_mae: 547950.3750\n",
      "Epoch 59/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1787456149627.7048 - mae: 567089.0625 - val_loss: 1016986240874.3979 - val_mae: 547191.0000\n",
      "Epoch 60/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 1785253181838.6040 - mae: 565538.6875 - val_loss: 1015009900322.1763 - val_mae: 546085.8750\n",
      "Epoch 61/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 1783253009483.5974 - mae: 564144.3125 - val_loss: 1015033341905.5718 - val_mae: 546094.8125\n",
      "Epoch 62/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1782290429910.7649 - mae: 561491.6250 - val_loss: 1014558424221.3401 - val_mae: 545468.0000\n",
      "Epoch 63/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1779455602055.7314 - mae: 561923.0000 - val_loss: 1014560255350.0050 - val_mae: 545437.4375\n",
      "Epoch 64/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1777740231343.2483 - mae: 562867.6250 - val_loss: 1015838712488.9471 - val_mae: 546006.4375\n",
      "Epoch 65/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 1776726159167.5703 - mae: 564049.5625 - val_loss: 1016364387351.2141 - val_mae: 545933.7500\n",
      "Epoch 66/100\n",
      "2384/2384 [==============================] - 0s 90us/step - loss: 1772568665672.1611 - mae: 564615.7500 - val_loss: 1015387347568.2015 - val_mae: 545514.4375\n",
      "Epoch 67/100\n",
      "2384/2384 [==============================] - 0s 90us/step - loss: 1772417612614.4429 - mae: 561029.6875 - val_loss: 1013787064758.4886 - val_mae: 543964.1250\n",
      "Epoch 68/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1771993487483.7048 - mae: 560270.2500 - val_loss: 1013338195034.2771 - val_mae: 543254.4375\n",
      "Epoch 69/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 1771149785376.6443 - mae: 559604.2500 - val_loss: 1016071855653.4005 - val_mae: 544498.3125\n",
      "Epoch 70/100\n",
      "2384/2384 [==============================] - 0s 88us/step - loss: 1768722565271.1946 - mae: 559636.8750 - val_loss: 1016040460071.3350 - val_mae: 544382.3750\n",
      "Epoch 71/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1767423347169.0737 - mae: 560912.4375 - val_loss: 1015798245618.4585 - val_mae: 543750.6250\n",
      "Epoch 72/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1766798183760.7517 - mae: 557537.2500 - val_loss: 1015637826944.3224 - val_mae: 543672.0000\n",
      "Epoch 73/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1762640210016.2148 - mae: 558254.9375 - val_loss: 1017601647438.0251 - val_mae: 544725.8125\n",
      "Epoch 74/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1763481999373.7451 - mae: 559682.1250 - val_loss: 1017102424770.7406 - val_mae: 544068.8125\n",
      "Epoch 75/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1761560390278.0134 - mae: 559558.5000 - val_loss: 1014589048228.4332 - val_mae: 541742.6875\n",
      "Epoch 76/100\n",
      "2384/2384 [==============================] - 0s 90us/step - loss: 1763777511863.8389 - mae: 555520.6875 - val_loss: 1016197922320.7657 - val_mae: 542354.6875\n",
      "Epoch 77/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1762244390829.5303 - mae: 556660.5625 - val_loss: 1016881702865.5718 - val_mae: 542588.0000\n",
      "Epoch 78/100\n",
      "2384/2384 [==============================] - 0s 106us/step - loss: 1760119465860.2952 - mae: 556517.9375 - val_loss: 1016811032400.6045 - val_mae: 542854.4375\n",
      "Epoch 79/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1759363169644.2417 - mae: 554169.5000 - val_loss: 1014892733303.2947 - val_mae: 541882.6875\n",
      "Epoch 80/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1759005102052.5100 - mae: 554265.6875 - val_loss: 1016325933855.5969 - val_mae: 541958.8125\n",
      "Epoch 81/100\n",
      "2384/2384 [==============================] - 0s 105us/step - loss: 1760652444637.6375 - mae: 555883.0625 - val_loss: 1018743080851.6675 - val_mae: 542536.6875\n",
      "Epoch 82/100\n",
      "2384/2384 [==============================] - 0s 104us/step - loss: 1757053455586.7920 - mae: 555163.1250 - val_loss: 1018430051614.3073 - val_mae: 542048.7500\n",
      "Epoch 83/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 1757992352692.4026 - mae: 552832.3125 - val_loss: 1018497213385.8337 - val_mae: 541485.7500\n",
      "Epoch 84/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1756936269521.6108 - mae: 553354.1250 - val_loss: 1021480250700.7355 - val_mae: 543046.8125\n",
      "Epoch 85/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 1756705065111.1946 - mae: 555789.6250 - val_loss: 1021739083110.5289 - val_mae: 542585.5000\n",
      "Epoch 86/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1756612938648.9126 - mae: 556791.0625 - val_loss: 1022848197247.6776 - val_mae: 542674.6875\n",
      "Epoch 87/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1753767931601.6108 - mae: 553476.0000 - val_loss: 1024145891472.4434 - val_mae: 543145.2500\n",
      "Epoch 88/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1755591019801.7720 - mae: 553178.5000 - val_loss: 1023875081698.3375 - val_mae: 542568.5625\n",
      "Epoch 89/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1753275076250.6309 - mae: 553593.3125 - val_loss: 1026740893350.3678 - val_mae: 544060.7500\n",
      "Epoch 90/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1754171262907.2751 - mae: 555806.1250 - val_loss: 1026897937044.3124 - val_mae: 543927.1250\n",
      "Epoch 91/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1753159255665.3960 - mae: 556215.0625 - val_loss: 1026560369774.9119 - val_mae: 543311.5625\n",
      "Epoch 92/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 1752638704426.9529 - mae: 555739.1875 - val_loss: 1026465229785.3098 - val_mae: 542950.2500\n",
      "Epoch 93/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1750609514358.5505 - mae: 551726.8125 - val_loss: 1027354489915.3250 - val_mae: 543154.1250\n",
      "Epoch 94/100\n",
      "2384/2384 [==============================] - 0s 102us/step - loss: 1748899342212.2952 - mae: 554273.3125 - val_loss: 1024338330961.8942 - val_mae: 541068.2500\n",
      "Epoch 95/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1750028696830.2820 - mae: 552492.0000 - val_loss: 1024589005460.3124 - val_mae: 541570.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1748979080831.1411 - mae: 551366.5000 - val_loss: 1025574490927.0730 - val_mae: 541939.4375\n",
      "Epoch 97/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1747641682620.9934 - mae: 551613.8750 - val_loss: 1023841216094.1461 - val_mae: 540899.5625\n",
      "Epoch 98/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1746455376889.1274 - mae: 551646.9375 - val_loss: 1027011518402.0957 - val_mae: 542615.1250\n",
      "Epoch 99/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1746489420312.0537 - mae: 552027.9375 - val_loss: 1027257356855.4559 - val_mae: 542266.3125\n",
      "Epoch 100/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1746480351493.1543 - mae: 552067.5000 - val_loss: 1026935893943.7783 - val_mae: 541686.8125\n",
      "794/794 [==============================] - 0s 33us/step\n",
      "3192/3192 [==============================] - 0s 34us/step\n",
      "processing fold # 1\n",
      "(794, 7)\n",
      "Train on 2384 samples, validate on 794 samples\n",
      "Epoch 1/100\n",
      "2384/2384 [==============================] - 1s 356us/step - loss: 2984684015148.6714 - mae: 875077.9375 - val_loss: 3399815291065.7129 - val_mae: 869676.1875\n",
      "Epoch 2/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 2984276679879.3022 - mae: 874921.5625 - val_loss: 3398985672327.4155 - val_mae: 869383.8750\n",
      "Epoch 3/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 2983099750083.8657 - mae: 874436.1250 - val_loss: 3396783375821.7026 - val_mae: 868637.2500\n",
      "Epoch 4/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 2979782851728.3223 - mae: 873283.1250 - val_loss: 3391793510673.4106 - val_mae: 867046.5000\n",
      "Epoch 5/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 2973716382355.7583 - mae: 871341.1250 - val_loss: 3383239313836.1714 - val_mae: 864672.3750\n",
      "Epoch 6/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 2964158808064.0000 - mae: 868784.6875 - val_loss: 3369899170498.7407 - val_mae: 861428.5625\n",
      "Epoch 7/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 2949819480682.5234 - mae: 865060.2500 - val_loss: 3349772646921.0278 - val_mae: 856893.3125\n",
      "Epoch 8/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 2928063360171.8120 - mae: 859668.6875 - val_loss: 3319865236534.1660 - val_mae: 850415.0000\n",
      "Epoch 9/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 2897144041884.3491 - mae: 852165.5000 - val_loss: 3280711553962.8818 - val_mae: 841832.6875\n",
      "Epoch 10/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 2853061247305.8794 - mae: 841994.5625 - val_loss: 3224152392629.1992 - val_mae: 829466.6250\n",
      "Epoch 11/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 2802106415502.6040 - mae: 828547.1875 - val_loss: 3160236735067.5669 - val_mae: 815115.1250\n",
      "Epoch 12/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 2739390098383.8926 - mae: 812062.8125 - val_loss: 3080510409544.8667 - val_mae: 796873.2500\n",
      "Epoch 13/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 2660585548531.9731 - mae: 791618.3125 - val_loss: 2981345983604.0703 - val_mae: 773674.8125\n",
      "Epoch 14/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 2563822486603.5977 - mae: 765901.2500 - val_loss: 2872806582349.3804 - val_mae: 747212.2500\n",
      "Epoch 15/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 2468854627912.1611 - mae: 738302.3750 - val_loss: 2747224014394.0352 - val_mae: 715644.3125\n",
      "Epoch 16/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 2352698279977.2349 - mae: 704246.6875 - val_loss: 2609967573564.6147 - val_mae: 680130.1875\n",
      "Epoch 17/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 2239406474095.6777 - mae: 669102.3125 - val_loss: 2477728210675.7480 - val_mae: 646655.6875\n",
      "Epoch 18/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 2124543906644.1880 - mae: 635545.0625 - val_loss: 2336462614125.6221 - val_mae: 614038.7500\n",
      "Epoch 19/100\n",
      "2384/2384 [==============================] - 0s 102us/step - loss: 2022622984260.7251 - mae: 607457.6875 - val_loss: 2202061351979.8486 - val_mae: 587552.0625\n",
      "Epoch 20/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 1928550054176.6443 - mae: 584486.3750 - val_loss: 2091358668113.8943 - val_mae: 569390.6250\n",
      "Epoch 21/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1833320871784.8054 - mae: 568840.3125 - val_loss: 1981493379484.6953 - val_mae: 557923.6875\n",
      "Epoch 22/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1772971907195.7048 - mae: 561460.8125 - val_loss: 1904923054110.9521 - val_mae: 553832.4375\n",
      "Epoch 23/100\n",
      "2384/2384 [==============================] - 0s 104us/step - loss: 1731870645447.3020 - mae: 561185.1875 - val_loss: 1845675013096.7859 - val_mae: 553828.8125\n",
      "Epoch 24/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1703126717927.9463 - mae: 565670.5625 - val_loss: 1807407651275.1235 - val_mae: 556122.0625\n",
      "Epoch 25/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1683131136288.6443 - mae: 568177.2500 - val_loss: 1778216885137.0881 - val_mae: 558933.3750\n",
      "Epoch 26/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 1670615460266.0940 - mae: 573624.5625 - val_loss: 1761734268809.3501 - val_mae: 560158.3125\n",
      "Epoch 27/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1660879795742.9263 - mae: 573921.3750 - val_loss: 1743387340810.3174 - val_mae: 561767.6250\n",
      "Epoch 28/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1653615992117.2617 - mae: 576589.1250 - val_loss: 1727966779108.2720 - val_mae: 563203.2500\n",
      "Epoch 29/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1646865419930.6309 - mae: 578851.6875 - val_loss: 1716105607647.7583 - val_mae: 563776.0000\n",
      "Epoch 30/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 1642067871235.4363 - mae: 580599.6875 - val_loss: 1705730992411.7280 - val_mae: 563988.1250\n",
      "Epoch 31/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 1636482765872.1072 - mae: 580016.0625 - val_loss: 1695882626303.3552 - val_mae: 563895.8750\n",
      "Epoch 32/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1630970090873.9866 - mae: 579171.0625 - val_loss: 1685111353189.2393 - val_mae: 564782.9375\n",
      "Epoch 33/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1627261817464.2686 - mae: 580396.7500 - val_loss: 1680360106432.8062 - val_mae: 563423.8125\n",
      "Epoch 34/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1623591742086.0134 - mae: 577208.8125 - val_loss: 1671206257836.8162 - val_mae: 563878.3750\n",
      "Epoch 35/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1619669581164.2417 - mae: 577368.4375 - val_loss: 1670634336225.0479 - val_mae: 561492.8750\n",
      "Epoch 36/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1617141506212.9397 - mae: 576860.5625 - val_loss: 1667373025831.9797 - val_mae: 560056.0000\n",
      "Epoch 37/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1613503190943.7852 - mae: 575439.5625 - val_loss: 1663295388127.7583 - val_mae: 559313.5625\n",
      "Epoch 38/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1611334160026.6309 - mae: 575580.5625 - val_loss: 1659979490373.6423 - val_mae: 558274.5625\n",
      "Epoch 39/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1606767363697.3960 - mae: 572747.4375 - val_loss: 1654187408458.8010 - val_mae: 558211.0000\n",
      "Epoch 40/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 1605858214107.9194 - mae: 572196.7500 - val_loss: 1651697931880.4634 - val_mae: 557009.6250\n",
      "Epoch 41/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1602516207217.3960 - mae: 572909.9375 - val_loss: 1649781311586.0151 - val_mae: 555765.3750\n",
      "Epoch 42/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1599411882794.9529 - mae: 568860.3750 - val_loss: 1646425129496.5037 - val_mae: 554975.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1597173640363.8120 - mae: 569192.1875 - val_loss: 1650045204423.2544 - val_mae: 552380.1875\n",
      "Epoch 44/100\n",
      "2384/2384 [==============================] - 0s 89us/step - loss: 1595725501048.2686 - mae: 566654.9375 - val_loss: 1645501088734.4685 - val_mae: 551911.5625\n",
      "Epoch 45/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1592707833691.0603 - mae: 566323.9375 - val_loss: 1644748858272.5642 - val_mae: 550860.0000\n",
      "Epoch 46/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1590541970933.6912 - mae: 566144.9375 - val_loss: 1640626817864.8665 - val_mae: 550681.5000\n",
      "Epoch 47/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1587379289672.1611 - mae: 565197.1875 - val_loss: 1640261744046.7507 - val_mae: 549422.2500\n",
      "Epoch 48/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1584749644367.0334 - mae: 563773.0625 - val_loss: 1640212816222.7910 - val_mae: 548033.8125\n",
      "Epoch 49/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1582345513825.9329 - mae: 562540.2500 - val_loss: 1634969847080.6248 - val_mae: 548246.2500\n",
      "Epoch 50/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1579542229019.4900 - mae: 561344.5625 - val_loss: 1627412036146.2971 - val_mae: 549122.0625\n",
      "Epoch 51/100\n",
      "2384/2384 [==============================] - 0s 94us/step - loss: 1577223645939.9731 - mae: 563906.1250 - val_loss: 1630374744053.6826 - val_mae: 546727.9375\n",
      "Epoch 52/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1575716932284.9934 - mae: 560337.1250 - val_loss: 1624884512040.6248 - val_mae: 547197.1875\n",
      "Epoch 53/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1574683808706.1477 - mae: 563076.7500 - val_loss: 1622381703425.9346 - val_mae: 546649.0625\n",
      "Epoch 54/100\n",
      "2384/2384 [==============================] - 0s 92us/step - loss: 1573181502787.0066 - mae: 561345.5000 - val_loss: 1617628370082.4988 - val_mae: 546911.9375\n",
      "Epoch 55/100\n",
      "2384/2384 [==============================] - 0s 90us/step - loss: 1570534736421.7988 - mae: 559750.3125 - val_loss: 1615363532753.5718 - val_mae: 546704.0625\n",
      "Epoch 56/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1568838673758.4966 - mae: 560570.7500 - val_loss: 1615089744145.4106 - val_mae: 545746.9375\n",
      "Epoch 57/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1566839589125.1543 - mae: 557816.7500 - val_loss: 1614169546401.2090 - val_mae: 544744.3125\n",
      "Epoch 58/100\n",
      "2384/2384 [==============================] - 0s 93us/step - loss: 1565744151586.3625 - mae: 558388.7500 - val_loss: 1610626523651.8689 - val_mae: 544978.5000\n",
      "Epoch 59/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1563698678900.8323 - mae: 560124.6875 - val_loss: 1608218782846.3879 - val_mae: 544616.2500\n",
      "Epoch 60/100\n",
      "2384/2384 [==============================] - 0s 90us/step - loss: 1561307945193.6643 - mae: 557284.6250 - val_loss: 1609452227153.2493 - val_mae: 542880.3750\n",
      "Epoch 61/100\n",
      "2384/2384 [==============================] - 0s 91us/step - loss: 1559644466677.6912 - mae: 555043.1250 - val_loss: 1603864804096.6448 - val_mae: 543886.3750\n",
      "Epoch 62/100\n",
      "2384/2384 [==============================] - 0s 103us/step - loss: 1558091267876.0806 - mae: 559169.6250 - val_loss: 1600816214975.5164 - val_mae: 543672.2500\n",
      "Epoch 63/100\n",
      "2384/2384 [==============================] - 0s 111us/step - loss: 1555960715827.5435 - mae: 556463.7500 - val_loss: 1599753288737.5315 - val_mae: 542889.1250\n",
      "Epoch 64/100\n",
      "2384/2384 [==============================] - 0s 110us/step - loss: 1553761184170.0940 - mae: 554268.0000 - val_loss: 1598564651575.4558 - val_mae: 542286.2500\n",
      "Epoch 65/100\n",
      "2384/2384 [==============================] - 0s 107us/step - loss: 1553535476096.8589 - mae: 555967.8125 - val_loss: 1597301937273.2292 - val_mae: 541559.6250\n",
      "Epoch 66/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 1552601760293.7988 - mae: 553821.8750 - val_loss: 1598039977622.8916 - val_mae: 540135.1250\n",
      "Epoch 67/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1550210070156.8857 - mae: 552300.7500 - val_loss: 1596781008612.2720 - val_mae: 539749.1250\n",
      "Epoch 68/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1549191650173.4229 - mae: 552641.0000 - val_loss: 1594895714195.6675 - val_mae: 539520.1250\n",
      "Epoch 69/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1548070093899.5974 - mae: 552559.6875 - val_loss: 1593093044332.3325 - val_mae: 539179.4375\n",
      "Epoch 70/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1545761610401.5034 - mae: 551659.8750 - val_loss: 1590435720318.3879 - val_mae: 539459.3750\n",
      "Epoch 71/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 1545155760760.2686 - mae: 552898.1250 - val_loss: 1586280767962.5996 - val_mae: 540186.7500\n",
      "Epoch 72/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1544460409285.5840 - mae: 553368.3125 - val_loss: 1584310293914.1160 - val_mae: 540080.5625\n",
      "Epoch 73/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1544399835768.2686 - mae: 550813.0625 - val_loss: 1585176421651.9900 - val_mae: 539020.7500\n",
      "Epoch 74/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1541276158652.9934 - mae: 551806.0625 - val_loss: 1579704226400.7253 - val_mae: 540393.3750\n",
      "Epoch 75/100\n",
      "2384/2384 [==============================] - 0s 104us/step - loss: 1539437366058.9529 - mae: 551881.9375 - val_loss: 1579903183792.0403 - val_mae: 539358.6250\n",
      "Epoch 76/100\n",
      "2384/2384 [==============================] - 0s 101us/step - loss: 1540033095776.2148 - mae: 552786.0000 - val_loss: 1577471465394.6196 - val_mae: 539581.8125\n",
      "Epoch 77/100\n",
      "2384/2384 [==============================] - 0s 97us/step - loss: 1538261113175.6243 - mae: 553486.9375 - val_loss: 1576578885874.4585 - val_mae: 538871.3125\n",
      "Epoch 78/100\n",
      "2384/2384 [==============================] - 0s 99us/step - loss: 1536718528347.0603 - mae: 551289.1875 - val_loss: 1574076416990.4685 - val_mae: 539170.4375\n",
      "Epoch 79/100\n",
      "2384/2384 [==============================] - 0s 100us/step - loss: 1536526893605.7988 - mae: 551154.8750 - val_loss: 1574969463681.6121 - val_mae: 537959.6250\n",
      "Epoch 80/100\n",
      "2384/2384 [==============================] - 0s 96us/step - loss: 1533747862837.2617 - mae: 552023.4375 - val_loss: 1578032918414.5088 - val_mae: 535876.5625\n",
      "Epoch 81/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1534542397330.0403 - mae: 549003.9375 - val_loss: 1574685416460.8967 - val_mae: 536387.1250\n",
      "Epoch 82/100\n",
      "2384/2384 [==============================] - 0s 95us/step - loss: 1531898480571.2751 - mae: 549301.1875 - val_loss: 1573025749120.9673 - val_mae: 536444.0625\n",
      "Epoch 83/100\n",
      "2384/2384 [==============================] - 0s 105us/step - loss: 1531607491604.6174 - mae: 548972.0625 - val_loss: 1571183413304.7456 - val_mae: 536529.4375\n",
      "Epoch 84/100\n",
      "2384/2384 [==============================] - 0s 102us/step - loss: 1530186064806.6577 - mae: 548668.3750 - val_loss: 1568740765747.5869 - val_mae: 537158.3125\n",
      "Epoch 85/100\n",
      "2384/2384 [==============================] - 0s 98us/step - loss: 1529304573168.5369 - mae: 548899.0625 - val_loss: 1568839393300.6348 - val_mae: 536251.8125\n",
      "Epoch 86/100\n",
      "  32/2384 [..............................] - ETA: 0s - loss: 1283678535680.0000 - mae: 741841.9375"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    print(train_data[:i * num_val_samples].shape)\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "        train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "        train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
    "    test_mse, test_mae = model.evaluate(test_data, test_targets, verbose =1 ) \n",
    "    #mae_history = history.history['val_mean_absolute_error']\n",
    "    #all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Discussion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
