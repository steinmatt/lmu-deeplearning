{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC 598 : Deep Learning Applications \n",
    "# Project 1 - Predicting Housing Prices \n",
    "### By: Matthew Stein and Ethan Tsao \n",
    "### February 10, 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "Data is imported via pandas, and processed via numpy. Working with Keras, all data is processed as arrays in numpy. To simplify model learning, feature-wise normalization is performed. Note that feature-wise normalization is calc'ed using the mean and std of the training set. \n",
    "\n",
    "The test set was chosen to be 15% of the overall data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SquareFootageStructure  LotSquareFootage  YearBuilt  Bedrooms  \\\n",
      "0                     1549.0            5825.0     1974.0       3.0   \n",
      "1                     1196.0            7900.0     1981.0       3.0   \n",
      "3                     3884.0           16013.0     1978.0       3.0   \n",
      "6                     1164.0            6611.0     1961.0       3.0   \n",
      "12                    2766.0            7331.0     1986.0       4.0   \n",
      "...                      ...               ...        ...       ...   \n",
      "9989                  2300.0            6547.0     1924.0       4.0   \n",
      "9990                  1545.0            2178.0     1967.0       2.0   \n",
      "9991                  1404.0            7841.0     1964.0       2.0   \n",
      "9993                   900.0            6000.0     1924.0       2.0   \n",
      "9999                  2865.0            7841.0     2005.0       4.0   \n",
      "\n",
      "      BathsTotal  field_StoriesTotal  field_PostalCode  ListPrice  \n",
      "0            2.0                 1.0           92624.0      3.300  \n",
      "1            2.0                 1.0           92316.0      1.600  \n",
      "3            3.0                 1.0           90274.0   2599.000  \n",
      "6            2.0                 1.0           91732.0    499.999  \n",
      "12           3.0                 2.0           91750.0    929.800  \n",
      "...          ...                 ...               ...        ...  \n",
      "9989         2.0                 1.0           90046.0   2445.000  \n",
      "9990         2.0                 1.0           92264.0    320.000  \n",
      "9991         2.0                 1.0           92586.0    228.500  \n",
      "9993         1.0                 1.0           90019.0      2.700  \n",
      "9999         3.0                 2.0           92585.0    399.990  \n",
      "\n",
      "[6370 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('data/resd_data.xlsx')\n",
    "length = data.shape[0]\n",
    "\n",
    "# Remove all rows with null points in dataset \n",
    "updated_data = data.dropna(how='any',axis=0) \n",
    "\n",
    "\n",
    "# Remove all rows with \"0\" for the Postal Code, \"0\" for Year Built, \"0\" for Lot Square Footage, and \"0\" for List Price\n",
    "updated_data = updated_data[updated_data.field_PostalCode != 0 ]\n",
    "updated_data = updated_data[updated_data.YearBuilt != 0]\n",
    "updated_data = updated_data[updated_data.LotSquareFootage != 0]\n",
    "updated_data = updated_data[updated_data.ListPrice != 0]\n",
    "\n",
    "updated_data['ListPrice'] = updated_data['ListPrice']/1000 \n",
    "\n",
    "# Update Index column \n",
    "\n",
    "print(updated_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = updated_data.loc[:(6370*.85),:'field_PostalCode'].values.tolist()\n",
    "train_data = np.asarray(train_data) \n",
    "train_targets = updated_data.loc[:(6370*.85),'ListPrice'].values.tolist()\n",
    "train_targets = np.asarray(train_targets) \n",
    "\n",
    "test_data = updated_data.loc[(6370*.85 +1 ):,:'field_PostalCode'].values.tolist()\n",
    "test_data = np.asarray(test_data) \n",
    "test_targets = updated_data.loc[(6370*.85 + 1):,'ListPrice'].values.tolist()\n",
    "test_targets = np.asarray(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection \n",
    "\n",
    "As is standard for neural networks - and regression models, in particular - a Sequential model was chosen. The model has 3 hidden layers, with 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-5e1025a3d2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "                           input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "(0, 7)\n",
      "Train on 1723 samples, validate on 1722 samples\n",
      "Epoch 1/100\n",
      "1723/1723 [==============================] - 1s 607us/step - loss: 3585124.6162 - mae: 865.9322 - val_loss: 2695631.2949 - val_mae: 805.0233\n",
      "Epoch 2/100\n",
      "1723/1723 [==============================] - 0s 129us/step - loss: 2935374.4048 - mae: 694.8428 - val_loss: 1857640.8595 - val_mae: 600.0143\n",
      "Epoch 3/100\n",
      "1723/1723 [==============================] - 0s 128us/step - loss: 2420348.7830 - mae: 604.6332 - val_loss: 1615223.0575 - val_mae: 590.6235\n",
      "Epoch 4/100\n",
      "1723/1723 [==============================] - 0s 146us/step - loss: 2258982.6111 - mae: 605.2721 - val_loss: 1507130.4241 - val_mae: 588.4382\n",
      "Epoch 5/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 2184732.2699 - mae: 597.2110 - val_loss: 1469044.8773 - val_mae: 577.4031\n",
      "Epoch 6/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 2155337.9615 - mae: 609.1622 - val_loss: 1446212.0136 - val_mae: 558.0206\n",
      "Epoch 7/100\n",
      "1723/1723 [==============================] - 0s 133us/step - loss: 2115331.4638 - mae: 582.5989 - val_loss: 1411211.6698 - val_mae: 560.6779\n",
      "Epoch 8/100\n",
      "1723/1723 [==============================] - 0s 127us/step - loss: 2072256.5984 - mae: 579.9018 - val_loss: 1403503.9399 - val_mae: 550.5178\n",
      "Epoch 9/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 2048188.2337 - mae: 572.3177 - val_loss: 1385243.4942 - val_mae: 549.1946\n",
      "Epoch 10/100\n",
      "1723/1723 [==============================] - 0s 128us/step - loss: 2025970.3110 - mae: 562.8740 - val_loss: 1364834.0940 - val_mae: 558.4955\n",
      "Epoch 11/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 2015797.3553 - mae: 579.8461 - val_loss: 1359750.2000 - val_mae: 545.9650\n",
      "Epoch 12/100\n",
      "1723/1723 [==============================] - 0s 149us/step - loss: 2012752.6212 - mae: 567.4113 - val_loss: 1350346.9208 - val_mae: 546.8430\n",
      "Epoch 13/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1966979.9144 - mae: 572.4541 - val_loss: 1360166.8563 - val_mae: 533.0814\n",
      "Epoch 14/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1972217.6014 - mae: 549.5738 - val_loss: 1336205.2307 - val_mae: 542.8830\n",
      "Epoch 15/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 1946739.8995 - mae: 552.5161 - val_loss: 1330875.3080 - val_mae: 562.0977\n",
      "Epoch 16/100\n",
      "1723/1723 [==============================] - 0s 133us/step - loss: 1941233.7360 - mae: 569.6168 - val_loss: 1329050.4605 - val_mae: 535.4526\n",
      "Epoch 17/100\n",
      "1723/1723 [==============================] - 0s 131us/step - loss: 1917379.7219 - mae: 552.8169 - val_loss: 1322914.8039 - val_mae: 558.5782\n",
      "Epoch 18/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 1924051.4651 - mae: 563.7605 - val_loss: 1322996.0440 - val_mae: 533.6193\n",
      "Epoch 19/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1905563.6713 - mae: 550.5693 - val_loss: 1310146.2066 - val_mae: 548.3866\n",
      "Epoch 20/100\n",
      "1723/1723 [==============================] - 0s 133us/step - loss: 1896789.9392 - mae: 556.2666 - val_loss: 1316008.2993 - val_mae: 533.7674\n",
      "Epoch 21/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1866032.7814 - mae: 548.3271 - val_loss: 1306784.1464 - val_mae: 555.0460\n",
      "Epoch 22/100\n",
      "1723/1723 [==============================] - 0s 144us/step - loss: 1877661.5219 - mae: 557.5855 - val_loss: 1301906.4006 - val_mae: 539.0309\n",
      "Epoch 23/100\n",
      "1723/1723 [==============================] - 0s 129us/step - loss: 1865402.7224 - mae: 551.6817 - val_loss: 1297391.9748 - val_mae: 541.1230\n",
      "Epoch 24/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 1885591.0233 - mae: 546.8224 - val_loss: 1298764.0685 - val_mae: 537.7344\n",
      "Epoch 25/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 1836623.6054 - mae: 548.0068 - val_loss: 1304036.2678 - val_mae: 559.5036\n",
      "Epoch 26/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1847089.1592 - mae: 555.6499 - val_loss: 1292130.3074 - val_mae: 545.8961\n",
      "Epoch 27/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1818751.7082 - mae: 565.0598 - val_loss: 1296742.3716 - val_mae: 531.6413\n",
      "Epoch 28/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1814990.3538 - mae: 543.2224 - val_loss: 1292310.4219 - val_mae: 554.3005\n",
      "Epoch 29/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 1801480.0544 - mae: 557.9337 - val_loss: 1299773.1853 - val_mae: 529.7622\n",
      "Epoch 30/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1796842.8886 - mae: 548.9016 - val_loss: 1285678.2938 - val_mae: 544.5438\n",
      "Epoch 31/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 1801676.9842 - mae: 556.1348 - val_loss: 1282961.9070 - val_mae: 537.7534\n",
      "Epoch 32/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 1790588.0609 - mae: 542.4587 - val_loss: 1284077.9312 - val_mae: 544.7374\n",
      "Epoch 33/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1783254.7784 - mae: 544.0773 - val_loss: 1285417.1518 - val_mae: 548.0947\n",
      "Epoch 34/100\n",
      "1723/1723 [==============================] - 0s 126us/step - loss: 1758575.3439 - mae: 553.9474 - val_loss: 1283063.3180 - val_mae: 531.3927\n",
      "Epoch 35/100\n",
      "1723/1723 [==============================] - 0s 155us/step - loss: 1767995.4029 - mae: 540.0682 - val_loss: 1277264.2724 - val_mae: 544.7509\n",
      "Epoch 36/100\n",
      "1723/1723 [==============================] - 0s 132us/step - loss: 1772185.4612 - mae: 544.3486 - val_loss: 1276594.5436 - val_mae: 534.6337\n",
      "Epoch 37/100\n",
      "1723/1723 [==============================] - 0s 129us/step - loss: 1746148.7644 - mae: 539.1844 - val_loss: 1280217.2890 - val_mae: 549.2809\n",
      "Epoch 38/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1737631.9966 - mae: 547.1536 - val_loss: 1272911.7099 - val_mae: 541.9649\n",
      "Epoch 39/100\n",
      "1723/1723 [==============================] - 0s 129us/step - loss: 1725229.2212 - mae: 542.8491 - val_loss: 1274863.1153 - val_mae: 543.3534\n",
      "Epoch 40/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1732213.9442 - mae: 543.6392 - val_loss: 1269133.2985 - val_mae: 534.6365\n",
      "Epoch 41/100\n",
      "1723/1723 [==============================] - 0s 129us/step - loss: 1727646.5913 - mae: 538.1419 - val_loss: 1269792.0303 - val_mae: 534.1750\n",
      "Epoch 42/100\n",
      "1723/1723 [==============================] - 0s 132us/step - loss: 1723660.4455 - mae: 538.0134 - val_loss: 1266904.5456 - val_mae: 535.6875\n",
      "Epoch 43/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1703324.8036 - mae: 533.2163 - val_loss: 1276171.6104 - val_mae: 549.9531\n",
      "Epoch 44/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1705667.6535 - mae: 549.8796 - val_loss: 1263956.8990 - val_mae: 536.3740\n",
      "Epoch 45/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 1709476.6205 - mae: 532.1126 - val_loss: 1267815.3691 - val_mae: 538.0137\n",
      "Epoch 46/100\n",
      "1723/1723 [==============================] - 0s 133us/step - loss: 1688299.6276 - mae: 537.4988 - val_loss: 1263049.3101 - val_mae: 534.9278\n",
      "Epoch 47/100\n",
      "1723/1723 [==============================] - 0s 132us/step - loss: 1692244.1228 - mae: 530.1539 - val_loss: 1275395.7804 - val_mae: 553.1711\n",
      "Epoch 48/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1691115.3158 - mae: 541.4969 - val_loss: 1261805.0495 - val_mae: 533.5426\n",
      "Epoch 49/100\n",
      "1723/1723 [==============================] - 0s 138us/step - loss: 1681333.4911 - mae: 535.5971 - val_loss: 1264270.7404 - val_mae: 535.2521\n",
      "Epoch 50/100\n",
      "1723/1723 [==============================] - 0s 135us/step - loss: 1666904.5874 - mae: 538.5893 - val_loss: 1264202.7204 - val_mae: 528.2859\n",
      "Epoch 51/100\n",
      "1723/1723 [==============================] - 0s 135us/step - loss: 1683124.5318 - mae: 531.2459 - val_loss: 1260947.2889 - val_mae: 529.1786\n",
      "Epoch 52/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1677667.1630 - mae: 529.9634 - val_loss: 1254760.9145 - val_mae: 530.2388\n",
      "Epoch 53/100\n",
      "1723/1723 [==============================] - 0s 141us/step - loss: 1661137.6815 - mae: 530.3673 - val_loss: 1269694.1641 - val_mae: 546.3578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "1723/1723 [==============================] - 0s 128us/step - loss: 1657561.0179 - mae: 534.6322 - val_loss: 1258420.3684 - val_mae: 535.9515\n",
      "Epoch 55/100\n",
      "1723/1723 [==============================] - 0s 128us/step - loss: 1649873.4486 - mae: 531.9629 - val_loss: 1262156.2847 - val_mae: 537.2930\n",
      "Epoch 56/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1649215.4061 - mae: 531.8745 - val_loss: 1257733.7616 - val_mae: 533.5933\n",
      "Epoch 57/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 1644589.0511 - mae: 527.9104 - val_loss: 1254568.1806 - val_mae: 531.1855\n",
      "Epoch 58/100\n",
      "1723/1723 [==============================] - 0s 138us/step - loss: 1637358.2793 - mae: 523.1564 - val_loss: 1259319.7170 - val_mae: 537.9476\n",
      "Epoch 59/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1624540.4121 - mae: 529.4323 - val_loss: 1262593.6825 - val_mae: 532.9222\n",
      "Epoch 60/100\n",
      "1723/1723 [==============================] - 0s 128us/step - loss: 1577034.8141 - mae: 529.9177 - val_loss: 1265255.9384 - val_mae: 525.6907\n",
      "Epoch 61/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1620926.2129 - mae: 519.5250 - val_loss: 1258405.6092 - val_mae: 531.3129\n",
      "Epoch 62/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1595518.2398 - mae: 515.5566 - val_loss: 1282868.8146 - val_mae: 553.5137\n",
      "Epoch 63/100\n",
      "1723/1723 [==============================] - 0s 131us/step - loss: 1577030.1518 - mae: 529.4175 - val_loss: 1263697.9785 - val_mae: 529.1369\n",
      "Epoch 64/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 1593411.6908 - mae: 516.9771 - val_loss: 1283695.2076 - val_mae: 543.2965\n",
      "Epoch 65/100\n",
      "1723/1723 [==============================] - 0s 144us/step - loss: 1543555.4748 - mae: 522.9673 - val_loss: 1266910.3200 - val_mae: 523.2043\n",
      "Epoch 66/100\n",
      "1723/1723 [==============================] - 0s 138us/step - loss: 1579240.7641 - mae: 516.6366 - val_loss: 1261751.3973 - val_mae: 522.8342\n",
      "Epoch 67/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1567760.8788 - mae: 515.3904 - val_loss: 1266610.1715 - val_mae: 528.5085\n",
      "Epoch 68/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1568474.1288 - mae: 516.7056 - val_loss: 1265111.3850 - val_mae: 535.7723\n",
      "Epoch 69/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1563857.8229 - mae: 509.3216 - val_loss: 1289295.7042 - val_mae: 551.9900\n",
      "Epoch 70/100\n",
      "1723/1723 [==============================] - 0s 141us/step - loss: 1533270.9808 - mae: 524.0435 - val_loss: 1282455.1053 - val_mae: 528.0668\n",
      "Epoch 71/100\n",
      "1723/1723 [==============================] - 0s 132us/step - loss: 1531185.2760 - mae: 514.9286 - val_loss: 1273787.1947 - val_mae: 533.1508\n",
      "Epoch 72/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1549872.4759 - mae: 513.4248 - val_loss: 1273961.1897 - val_mae: 522.7627\n",
      "Epoch 73/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1525948.9646 - mae: 508.7321 - val_loss: 1303448.2425 - val_mae: 537.6518\n",
      "Epoch 74/100\n",
      "1723/1723 [==============================] - 0s 141us/step - loss: 1516645.0020 - mae: 513.4797 - val_loss: 1277225.5231 - val_mae: 527.2657\n",
      "Epoch 75/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1523590.8134 - mae: 508.3842 - val_loss: 1330585.1242 - val_mae: 553.6357\n",
      "Epoch 76/100\n",
      "1723/1723 [==============================] - 0s 133us/step - loss: 1496393.9703 - mae: 522.7501 - val_loss: 1288292.1858 - val_mae: 527.2250\n",
      "Epoch 77/100\n",
      "1723/1723 [==============================] - 0s 143us/step - loss: 1492150.0203 - mae: 508.1848 - val_loss: 1314939.2206 - val_mae: 532.7177\n",
      "Epoch 78/100\n",
      "1723/1723 [==============================] - 0s 161us/step - loss: 1516975.0916 - mae: 508.9106 - val_loss: 1292495.8027 - val_mae: 525.9064\n",
      "Epoch 79/100\n",
      "1723/1723 [==============================] - 0s 162us/step - loss: 1505086.9308 - mae: 510.0245 - val_loss: 1296138.5383 - val_mae: 522.3998\n",
      "Epoch 80/100\n",
      "1723/1723 [==============================] - 0s 173us/step - loss: 1487936.6539 - mae: 508.6695 - val_loss: 1295139.5989 - val_mae: 531.3705\n",
      "Epoch 81/100\n",
      "1723/1723 [==============================] - 0s 148us/step - loss: 1495317.7862 - mae: 503.9999 - val_loss: 1322876.4068 - val_mae: 541.9783\n",
      "Epoch 82/100\n",
      "1723/1723 [==============================] - 0s 153us/step - loss: 1481973.8635 - mae: 507.5255 - val_loss: 1310641.3664 - val_mae: 529.1517\n",
      "Epoch 83/100\n",
      "1723/1723 [==============================] - 0s 170us/step - loss: 1442780.5524 - mae: 505.7333 - val_loss: 1312995.4365 - val_mae: 525.4003\n",
      "Epoch 84/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1489259.4127 - mae: 501.9192 - val_loss: 1324168.0768 - val_mae: 534.8627\n",
      "Epoch 85/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1461861.2697 - mae: 499.8759 - val_loss: 1343276.9257 - val_mae: 546.9355\n",
      "Epoch 86/100\n",
      "1723/1723 [==============================] - 0s 146us/step - loss: 1466294.4686 - mae: 500.6958 - val_loss: 1378904.1771 - val_mae: 555.2618\n",
      "Epoch 87/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1459652.4082 - mae: 506.1617 - val_loss: 1365673.0622 - val_mae: 540.3885\n",
      "Epoch 88/100\n",
      "1723/1723 [==============================] - 0s 183us/step - loss: 1446725.7705 - mae: 503.7473 - val_loss: 1372113.4676 - val_mae: 532.5668\n",
      "Epoch 89/100\n",
      "1723/1723 [==============================] - 0s 145us/step - loss: 1440619.3955 - mae: 500.9613 - val_loss: 1345930.2420 - val_mae: 528.4205\n",
      "Epoch 90/100\n",
      "1723/1723 [==============================] - 0s 154us/step - loss: 1439906.8826 - mae: 497.6405 - val_loss: 1354376.1409 - val_mae: 529.7490\n",
      "Epoch 91/100\n",
      "1723/1723 [==============================] - 0s 141us/step - loss: 1435723.5643 - mae: 497.8527 - val_loss: 1353925.6316 - val_mae: 532.8074\n",
      "Epoch 92/100\n",
      "1723/1723 [==============================] - 0s 143us/step - loss: 1441297.9002 - mae: 493.2716 - val_loss: 1372469.3992 - val_mae: 536.0944\n",
      "Epoch 93/100\n",
      "1723/1723 [==============================] - 0s 145us/step - loss: 1426681.0391 - mae: 494.0211 - val_loss: 1362648.5891 - val_mae: 531.6456\n",
      "Epoch 94/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1434676.3479 - mae: 497.6741 - val_loss: 1364517.1871 - val_mae: 526.0359\n",
      "Epoch 95/100\n",
      "1723/1723 [==============================] - 0s 145us/step - loss: 1402723.2104 - mae: 496.9408 - val_loss: 1375153.5419 - val_mae: 533.2215\n",
      "Epoch 96/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1412798.2302 - mae: 496.8384 - val_loss: 1376297.4238 - val_mae: 524.6354\n",
      "Epoch 97/100\n",
      "1723/1723 [==============================] - 0s 147us/step - loss: 1414040.8745 - mae: 491.6046 - val_loss: 1394211.5785 - val_mae: 535.5253\n",
      "Epoch 98/100\n",
      "1723/1723 [==============================] - 0s 145us/step - loss: 1399219.5160 - mae: 493.4773 - val_loss: 1392684.0928 - val_mae: 522.5714\n",
      "Epoch 99/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1430269.2969 - mae: 483.4384 - val_loss: 1400494.3723 - val_mae: 535.3754\n",
      "Epoch 100/100\n",
      "1723/1723 [==============================] - 0s 144us/step - loss: 1402195.3544 - mae: 490.3703 - val_loss: 1410244.2344 - val_mae: 522.5158\n",
      "1722/1722 [==============================] - 0s 40us/step\n",
      "processing fold # 1\n",
      "(1722, 7)\n",
      "Train on 1723 samples, validate on 1722 samples\n",
      "Epoch 1/100\n",
      "1723/1723 [==============================] - 1s 670us/step - loss: 2842214.1965 - mae: 847.6796 - val_loss: 3408251.9738 - val_mae: 818.9091\n",
      "Epoch 2/100\n",
      "1723/1723 [==============================] - 0s 127us/step - loss: 2341960.0695 - mae: 697.9487 - val_loss: 2608951.4630 - val_mae: 608.5485\n",
      "Epoch 3/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1646072.0930 - mae: 599.5214 - val_loss: 2286784.1683 - val_mae: 595.8344\n",
      "Epoch 4/100\n",
      "1723/1723 [==============================] - 0s 138us/step - loss: 1511317.9064 - mae: 593.2738 - val_loss: 2244632.2408 - val_mae: 577.8954\n",
      "Epoch 5/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1453417.2474 - mae: 587.5367 - val_loss: 2199177.7617 - val_mae: 576.4835\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723/1723 [==============================] - 0s 143us/step - loss: 1437691.2843 - mae: 563.5240 - val_loss: 2179280.8540 - val_mae: 586.5085\n",
      "Epoch 7/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1416369.3718 - mae: 563.5542 - val_loss: 2158552.6356 - val_mae: 588.5964\n",
      "Epoch 8/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1396988.8692 - mae: 560.7629 - val_loss: 2145032.0407 - val_mae: 587.6395\n",
      "Epoch 9/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1379156.3600 - mae: 555.0150 - val_loss: 2135569.0640 - val_mae: 594.5838\n",
      "Epoch 10/100\n",
      "1723/1723 [==============================] - 0s 139us/step - loss: 1374330.7307 - mae: 554.9598 - val_loss: 2123071.1070 - val_mae: 582.2834\n",
      "Epoch 11/100\n",
      "1723/1723 [==============================] - 0s 133us/step - loss: 1360351.3931 - mae: 549.1420 - val_loss: 2135744.1094 - val_mae: 607.3796\n",
      "Epoch 12/100\n",
      "1723/1723 [==============================] - 0s 137us/step - loss: 1341570.1723 - mae: 556.2477 - val_loss: 2130139.2909 - val_mae: 609.6262\n",
      "Epoch 13/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1339662.5999 - mae: 557.2861 - val_loss: 2090832.8003 - val_mae: 580.4236\n",
      "Epoch 14/100\n",
      "1723/1723 [==============================] - 0s 133us/step - loss: 1333378.9361 - mae: 550.3916 - val_loss: 2086635.4816 - val_mae: 567.3337\n",
      "Epoch 15/100\n",
      "1723/1723 [==============================] - 0s 150us/step - loss: 1320250.8006 - mae: 551.5883 - val_loss: 2077739.3153 - val_mae: 566.9696\n",
      "Epoch 16/100\n",
      "1723/1723 [==============================] - 0s 144us/step - loss: 1319252.7165 - mae: 537.8028 - val_loss: 2086723.8516 - val_mae: 595.3444\n",
      "Epoch 17/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 1317336.1084 - mae: 551.0480 - val_loss: 2066641.6289 - val_mae: 585.7367\n",
      "Epoch 18/100\n",
      "1723/1723 [==============================] - 0s 130us/step - loss: 1319385.1108 - mae: 544.6815 - val_loss: 2063287.4058 - val_mae: 582.5815\n",
      "Epoch 19/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1293356.7714 - mae: 540.1343 - val_loss: 2093791.4708 - val_mae: 619.1897\n",
      "Epoch 20/100\n",
      "1723/1723 [==============================] - 0s 156us/step - loss: 1305207.9271 - mae: 548.8571 - val_loss: 2064476.1171 - val_mae: 602.7980\n",
      "Epoch 21/100\n",
      "1723/1723 [==============================] - 0s 134us/step - loss: 1271223.7163 - mae: 555.2526 - val_loss: 2052371.9498 - val_mae: 559.1928\n",
      "Epoch 22/100\n",
      "1723/1723 [==============================] - 0s 131us/step - loss: 1286047.4849 - mae: 543.3831 - val_loss: 2039700.9067 - val_mae: 565.3369\n",
      "Epoch 23/100\n",
      "1723/1723 [==============================] - 0s 138us/step - loss: 1294859.5901 - mae: 536.3780 - val_loss: 2032658.3516 - val_mae: 579.8438\n",
      "Epoch 24/100\n",
      "1723/1723 [==============================] - 0s 132us/step - loss: 1289173.5649 - mae: 538.7230 - val_loss: 2031288.7890 - val_mae: 573.6285\n",
      "Epoch 25/100\n",
      "1723/1723 [==============================] - 0s 154us/step - loss: 1277889.0675 - mae: 545.8394 - val_loss: 2023284.7178 - val_mae: 570.9051\n",
      "Epoch 26/100\n",
      "1723/1723 [==============================] - 0s 153us/step - loss: 1287689.4047 - mae: 532.4841 - val_loss: 2028453.4964 - val_mae: 586.3966\n",
      "Epoch 27/100\n",
      "1723/1723 [==============================] - 0s 155us/step - loss: 1276315.4227 - mae: 543.9852 - val_loss: 2010449.1774 - val_mae: 567.9237\n",
      "Epoch 28/100\n",
      "1723/1723 [==============================] - 0s 153us/step - loss: 1271205.4066 - mae: 539.9172 - val_loss: 2008807.1705 - val_mae: 569.4057\n",
      "Epoch 29/100\n",
      "1723/1723 [==============================] - 0s 151us/step - loss: 1263294.1897 - mae: 536.8064 - val_loss: 2009371.5066 - val_mae: 572.8339\n",
      "Epoch 30/100\n",
      "1723/1723 [==============================] - 0s 149us/step - loss: 1268082.4464 - mae: 539.0956 - val_loss: 2002316.3200 - val_mae: 574.5859\n",
      "Epoch 31/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 1268639.8828 - mae: 537.2249 - val_loss: 1999964.8455 - val_mae: 565.0732\n",
      "Epoch 32/100\n",
      "1723/1723 [==============================] - 0s 161us/step - loss: 1261383.7525 - mae: 532.0601 - val_loss: 1999712.2097 - val_mae: 577.8551\n",
      "Epoch 33/100\n",
      "1723/1723 [==============================] - 0s 144us/step - loss: 1250707.4752 - mae: 536.7433 - val_loss: 1998212.8482 - val_mae: 581.9480\n",
      "Epoch 34/100\n",
      "1723/1723 [==============================] - 0s 136us/step - loss: 1258102.1841 - mae: 532.2849 - val_loss: 1986330.9637 - val_mae: 566.4758\n",
      "Epoch 35/100\n",
      "1723/1723 [==============================] - 0s 151us/step - loss: 1252915.1079 - mae: 527.4718 - val_loss: 1988668.6039 - val_mae: 569.3456\n",
      "Epoch 36/100\n",
      "1723/1723 [==============================] - 0s 156us/step - loss: 1243819.4790 - mae: 534.1832 - val_loss: 1992436.3870 - val_mae: 566.1320\n",
      "Epoch 37/100\n",
      "1723/1723 [==============================] - 0s 152us/step - loss: 1237956.0063 - mae: 537.5707 - val_loss: 1981894.6635 - val_mae: 564.0463\n",
      "Epoch 38/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1242807.7446 - mae: 525.8137 - val_loss: 1993683.4500 - val_mae: 586.3726\n",
      "Epoch 39/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1246629.7183 - mae: 530.7022 - val_loss: 1984589.5731 - val_mae: 558.2890\n",
      "Epoch 40/100\n",
      "1723/1723 [==============================] - 0s 151us/step - loss: 1236203.0832 - mae: 529.5356 - val_loss: 1986324.8647 - val_mae: 555.3018\n",
      "Epoch 41/100\n",
      "1723/1723 [==============================] - 0s 149us/step - loss: 1216238.9293 - mae: 529.3952 - val_loss: 2013523.0837 - val_mae: 546.3671\n",
      "Epoch 42/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1227736.3674 - mae: 519.3820 - val_loss: 1995251.8550 - val_mae: 585.5485\n",
      "Epoch 43/100\n",
      "1723/1723 [==============================] - 0s 148us/step - loss: 1221268.6156 - mae: 533.3438 - val_loss: 1975644.0765 - val_mae: 564.0805\n",
      "Epoch 44/100\n",
      "1723/1723 [==============================] - 0s 140us/step - loss: 1219309.0094 - mae: 530.3853 - val_loss: 1985058.3272 - val_mae: 566.7173\n",
      "Epoch 45/100\n",
      "1723/1723 [==============================] - 0s 147us/step - loss: 1206607.9448 - mae: 520.3574 - val_loss: 1986097.2255 - val_mae: 577.1819\n",
      "Epoch 46/100\n",
      "1723/1723 [==============================] - 0s 142us/step - loss: 1226348.6027 - mae: 523.6463 - val_loss: 1988912.4807 - val_mae: 564.1318\n",
      "Epoch 47/100\n",
      "1723/1723 [==============================] - 0s 144us/step - loss: 1213337.8192 - mae: 520.7766 - val_loss: 1998866.3437 - val_mae: 576.8519\n",
      "Epoch 48/100\n",
      "1723/1723 [==============================] - 0s 145us/step - loss: 1202117.9040 - mae: 525.8134 - val_loss: 1989077.0917 - val_mae: 564.3577\n",
      "Epoch 49/100\n",
      "1723/1723 [==============================] - 0s 148us/step - loss: 1206917.7268 - mae: 519.0455 - val_loss: 2008506.4660 - val_mae: 584.5429\n",
      "Epoch 50/100\n",
      "1723/1723 [==============================] - 0s 143us/step - loss: 1209987.1908 - mae: 525.1642 - val_loss: 2000493.2856 - val_mae: 564.4512\n",
      "Epoch 51/100\n",
      "1723/1723 [==============================] - 0s 147us/step - loss: 1187378.6905 - mae: 520.4582 - val_loss: 2009456.8115 - val_mae: 574.4106\n",
      "Epoch 52/100\n",
      "1723/1723 [==============================] - 0s 145us/step - loss: 1197542.2993 - mae: 520.6848 - val_loss: 2013982.5671 - val_mae: 559.1012\n",
      "Epoch 53/100\n",
      "1723/1723 [==============================] - 0s 146us/step - loss: 1186867.0128 - mae: 522.9286 - val_loss: 2021266.6779 - val_mae: 556.1454\n",
      "Epoch 54/100\n",
      "1723/1723 [==============================] - 0s 151us/step - loss: 1208822.5107 - mae: 513.1947 - val_loss: 2011220.7188 - val_mae: 555.0406\n",
      "Epoch 55/100\n",
      "1723/1723 [==============================] - 0s 147us/step - loss: 1169752.2775 - mae: 516.8286 - val_loss: 2029231.8058 - val_mae: 565.5392\n",
      "Epoch 56/100\n",
      "1723/1723 [==============================] - 0s 146us/step - loss: 1186833.7293 - mae: 508.7863 - val_loss: 2038944.6452 - val_mae: 569.9384\n",
      "Epoch 57/100\n",
      "1600/1723 [==========================>...] - ETA: 0s - loss: 1215820.2683 - mae: 518.3748"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-bd331123ba9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     history = model.fit(partial_train_data, partial_train_targets,\n\u001b[1;32m     26\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                         epochs=num_epochs, batch_size=32, verbose=1)\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mval_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#mae_history = history.history['val_mean_absolute_error']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                                          verbose=0)\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3745\u001b[0m     \u001b[0;31m# private numpy conversion method is preferred to guarantee performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m-> 3747\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3748\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3749\u001b[0m         expand_composites=True)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k=4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    print(train_data[:i * num_val_samples].shape)\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "        train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "        train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1) \n",
    "    #mae_history = history.history['val_mean_absolute_error']\n",
    "    #all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Discussion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
